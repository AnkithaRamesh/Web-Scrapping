{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Scraping Tutorial: Amazon Best Sellers</title>\n",
    "</head>\n",
    "\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body>\n",
    "    <h1>Web Scraping from Amazon with Python</h1>\n",
    "    <p>In this tutorial, I will walk through the process of web scraping from Amazon’s Best Sellers page in the Teaching & Education category to collect data about the top 50 authors and their ratings. Before we start, ensure you have the following Python libraries installed:</p>\n",
    "        <li><strong>requests:</strong> to send HTTP requests and retrieve the web pages</li>\n",
    "        <li><strong>BeautifulSoup:</strong> to parse and extract information from the HTML content.</li>\n",
    "        <li><strong>pandas:</strong> to organize and save the extracted data in a tabular format.</li>\n",
    "     \n",
    "<p> Pandas and requests are already available in your Python environment if you are using colab or jupyter notebook. Use the command below in your colab or jupyter notebook to install BeautifulSoup: </p>\n",
    "    \n",
    "<pre><code>!pip install beautifulsoup4</code></pre>\n",
    "\n",
    "<p>Now, let’s get started with web scraping from Amazon.</p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Scraping Tutorial: Amazon Best Sellers - Pagination</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Step 1: Understanding the Target URL and Pagination</h1>\n",
    "    <p>We are targeting the Amazon Best Sellers page in the Teaching & Education category. Amazon’s pagination allows us to navigate through multiple pages of results. The base URL for the first page looks like this:</p>\n",
    "   <pre><code>https://www.amazon.in/gp/bestsellers/books/4149461031/ref=zg_bs_pg_1?ie=UTF8&pg=1</code></pre>\n",
    "<p>Notice the pagination parameters “pg” and “zg_bs_pg” in the URL. We will increment these values to navigate through the pages.</p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1>Step 2: Set Up the HTTP Request</h1>\n",
    "\n",
    "<p>To scrape the content from Amazon, we first need to send a request to the server and retrieve the HTML content of the page. We also need to mimic a real browser to avoid being blocked by Amazon, which is why we always need to include a User-Agent header in the request. Here’s how to set up the HTTP request:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# base url of the best sellers page for teaching & education books\n",
    "base_url = \"https://www.amazon.in/gp/bestsellers/books/4149461031/ref=zg_bs_pg_{}?ie=UTF8&pg={}\"\n",
    "\n",
    "# http headers to mimic a browser visit\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 3: Iterate Over Pages to Collect Data </h1>\n",
    "<p> Now, we will loop through the first three pages to collect data for the top 50 books (assuming each page displays around 20 items). On each page, we will extract the author’s name and rating: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list to store book data\n",
    "book_list = []\n",
    "\n",
    "# iterate over the first 3 pages to get top 50 books (assuming each page has about 20 items)\n",
    "for page in range(1, 4):\n",
    "    # construct the URL for the current page\n",
    "    url = base_url.format(page, page)\n",
    "    \n",
    "    # send a GET request to the url\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # find all the book elements\n",
    "    books = soup.find_all(\"div\", {\"class\": \"zg-grid-general-faceout\"})\n",
    "    \n",
    "    # iterate over each book element to extract data\n",
    "    for book in books:\n",
    "        if len(book_list) < 50:  # stop once we've collected 50 books\n",
    "            author = book.find(\"a\", class_=\"a-size-small a-link-child\").get_text(strip=True) if book.find(\"a\", class_=\"a-size-small a-link-child\") else \"N/A\"\n",
    "            rating = book.find(\"span\", class_=\"a-icon-alt\").get_text(strip=True) if book.find(\"span\", class_=\"a-icon-alt\") else \"N/A\"\n",
    "            \n",
    "            # append the extracted data to the book_list\n",
    "            book_list.append({\n",
    "                \"Author\": author,\n",
    "                \"Rating\": rating\n",
    "            })\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we looped through the first three pages using a for loop. The condition <b>if len(book_list) < 50 </b>: ensures that we stop once we’ve collected data for 50 books. The code works by iterating through the first three pages of Amazon’s Best Sellers list in the Teaching & Education category. For each page, it sends a GET request to retrieve the HTML content, then parses this content using BeautifulSoup to find the relevant book elements. It extracts the author and rating for each book, appending the data to a list until it has collected information for 50 books.</P>\n",
    "\n",
    "<p>The loop breaks once 50 books have been processed, which ensures that only the top 50 authors and their ratings are captured.</P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4: Store and Save the Data</h1>\n",
    "\n",
    "<p>After collecting the data, we will store it in a Pandas DataFrame and save it to a CSV file:</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Author              Rating\n",
      "0  Samapti Sinha Mahapatra  4.6 out of 5 stars\n",
      "1             Akshay Kumar  4.3 out of 5 stars\n",
      "2                      N/A  4.4 out of 5 stars\n",
      "3            Lori Gottlieb  4.6 out of 5 stars\n",
      "4           एम लक्ष्मीकांत  4.4 out of 5 stars\n"
     ]
    }
   ],
   "source": [
    "# convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(book_list)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df.to_csv(\"amazon_top_50_books_authors_ratings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let’s have a look at some sample rows as well:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Author              Rating\n",
      "8   EduGorilla Prep Experts  3.5 out of 5 stars\n",
      "32      RPH Editorial Board  3.8 out of 5 stars\n",
      "16   EduGorilla PREP EXPERT  4.0 out of 5 stars\n",
      "47         Ronnie Screwvala  4.4 out of 5 stars\n",
      "20             Mike Chapple  4.7 out of 5 stars\n",
      "31               Pavan Soni  4.9 out of 5 stars\n",
      "25          Nikhil Kr Gupta  4.5 out of 5 stars\n",
      "38   ALLEN Expert Faculties                 N/A\n",
      "28                      N/A  4.3 out of 5 stars\n",
      "12             Kriti Sharma  4.6 out of 5 stars\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>This method can be adapted for different categories or more extensive data collection by adjusting the page range or the conditions within the loop.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Summary</h1>\n",
    "\n",
    "<p1>So, web scraping is a technique used to extract data from websites by sending requests to the server, retrieving the web pages, and parsing the HTML content to extract the necessary information.</p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
